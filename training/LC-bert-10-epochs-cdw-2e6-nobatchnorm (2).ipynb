{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Import the libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom nltk.tokenize import WordPunctTokenizer\nimport os\nimport urllib.request\nimport tensorflow as tf\nimport torch\nimport os\nimport torchvision\nimport tarfile\nfrom torch.utils.data import random_split\nfrom torchvision.datasets.utils import download_url\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport spacy\nfrom torch.utils.data import DataLoader","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:12:57.343863Z","iopub.execute_input":"2022-01-04T17:12:57.344154Z","iopub.status.idle":"2022-01-04T17:13:07.039539Z","shell.execute_reply.started":"2022-01-04T17:12:57.344122Z","shell.execute_reply":"2022-01-04T17:13:07.038782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import BertModel\n\nfrom transformers.models.bert.modeling_bert import BertPooler, BertSelfAttention\nfrom transformers import BertTokenizer\nfrom torch.utils.data import Dataset\npretrained_bert=\"bert-base-uncased\"","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:13:07.041384Z","iopub.execute_input":"2022-01-04T17:13:07.041636Z","iopub.status.idle":"2022-01-04T17:13:08.506089Z","shell.execute_reply.started":"2022-01-04T17:13:07.041602Z","shell.execute_reply":"2022-01-04T17:13:08.505356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install openpyxl","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:13:08.507217Z","iopub.execute_input":"2022-01-04T17:13:08.507481Z","iopub.status.idle":"2022-01-04T17:13:15.934872Z","shell.execute_reply.started":"2022-01-04T17:13:08.507448Z","shell.execute_reply":"2022-01-04T17:13:15.933914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking required packages for this notebook","metadata":{}},{"cell_type":"code","source":"import pkg_resources\nimport types\ndef get_imports():\n    for name, val in globals().items():\n        if isinstance(val, types.ModuleType):\n            # Split ensures you get root package, \n            # not just imported function\n            name = val.__name__.split(\".\")[0]\n\n        elif isinstance(val, type):\n            name = val.__module__.split(\".\")[0]\n\n        # Some packages are weird and have different\n        # imported names vs. system names\n        if name == \"PIL\":\n            name = \"Pillow\"\n        elif name == \"sklearn\":\n            name = \"scikit-learn\"\n\n        yield name\nimports = list(set(get_imports()))\n\nrequirements = []\nfor m in pkg_resources.working_set:\n    if m.project_name in imports and m.project_name!=\"pip\":\n        requirements.append((m.project_name, m.version))\n\nfor r in requirements:\n    print(\"{}=={}\".format(*r))","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:13:15.937877Z","iopub.execute_input":"2022-01-04T17:13:15.938173Z","iopub.status.idle":"2022-01-04T17:13:15.95103Z","shell.execute_reply.started":"2022-01-04T17:13:15.93814Z","shell.execute_reply":"2022-01-04T17:13:15.950039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=pd.read_excel('../input/enterpret-absa/train.xlsx')","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:13:15.952572Z","iopub.execute_input":"2022-01-04T17:13:15.953646Z","iopub.status.idle":"2022-01-04T17:13:16.611151Z","shell.execute_reply.started":"2022-01-04T17:13:15.953603Z","shell.execute_reply":"2022-01-04T17:13:16.61044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_data(data):\n    for i in range(len(data)):\n        if(type(data.loc[i,'aspect'])==float):\n            data.loc[i,'aspect']=str(data.loc[i,'aspect'])\n    data['text_tok'] = data['text'].apply(lambda x: x.lower())\n    data['text_tok'] = data['text_tok'].apply(custom_tokenize)\n    data['aspect_tok'] = data['aspect'].apply(lambda x: x.lower())\n    data['aspect_tok'] = data['aspect_tok'].apply(custom_tokenize)\n    return data\n\ndef custom_tokenize(text):#split snetences into a list of word\n    tokenizer = WordPunctTokenizer()\n    tokens = tokenizer.tokenize(text)\n    words = [word for word in tokens if word.isalnum()]\n    return words\n\ndef max_len(data):#function to find maximum length of text and aspect\n    max_text=0\n    max_asp=0\n    for i in range(len(data)):\n        max_text=max(max_text,len(data.loc[i,'text_tok']))\n        max_asp=max(max_asp,len(data.loc[i,'aspect_tok']))\n    return max_text,max_asp\n\n\n    \n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:13:16.612539Z","iopub.execute_input":"2022-01-04T17:13:16.612809Z","iopub.status.idle":"2022-01-04T17:13:16.6219Z","shell.execute_reply.started":"2022-01-04T17:13:16.612772Z","shell.execute_reply":"2022-01-04T17:13:16.620923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_2=train.copy()\ndata=parse_data(train_2)\n\n#print(data)\nmax_tex,max_asp=max_len(data)\n#vocab=find_w(data)\n#w_ind,i_w=create_dict(vocab,300)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:14:21.761155Z","iopub.execute_input":"2022-01-04T17:14:21.761917Z","iopub.status.idle":"2022-01-04T17:14:21.992745Z","shell.execute_reply.started":"2022-01-04T17:14:21.761879Z","shell.execute_reply":"2022-01-04T17:14:21.992035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tex=torch.LongTensor(tex)\n#aspec=torch.LongTensor(aspec)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:14:26.832892Z","iopub.execute_input":"2022-01-04T17:14:26.833798Z","iopub.status.idle":"2022-01-04T17:14:26.838776Z","shell.execute_reply.started":"2022-01-04T17:14:26.833755Z","shell.execute_reply":"2022-01-04T17:14:26.83724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['label']=data['label'].apply(lambda x:int(x))\nlabels= torch.tensor(data['label'].values)#data required in tensor form","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:14:27.107043Z","iopub.execute_input":"2022-01-04T17:14:27.107646Z","iopub.status.idle":"2022-01-04T17:14:27.115366Z","shell.execute_reply.started":"2022-01-04T17:14:27.107607Z","shell.execute_reply":"2022-01-04T17:14:27.114555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"\n#function to get the device being used.use CUDA for training and inferencing\n\ndef get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:14:27.657556Z","iopub.execute_input":"2022-01-04T17:14:27.658156Z","iopub.status.idle":"2022-01-04T17:14:27.665521Z","shell.execute_reply.started":"2022-01-04T17:14:27.658119Z","shell.execute_reply":"2022-01-04T17:14:27.664739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device=get_default_device()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:14:28.233635Z","iopub.execute_input":"2022-01-04T17:14:28.234091Z","iopub.status.idle":"2022-01-04T17:14:28.237577Z","shell.execute_reply.started":"2022-01-04T17:14:28.23405Z","shell.execute_reply":"2022-01-04T17:14:28.236813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion=nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:14:28.706727Z","iopub.execute_input":"2022-01-04T17:14:28.707414Z","iopub.status.idle":"2022-01-04T17:14:28.711846Z","shell.execute_reply.started":"2022-01-04T17:14:28.707372Z","shell.execute_reply":"2022-01-04T17:14:28.710877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AspectC(nn.Module):#utility class for training and testing.Needs to be present\n                          #in all notebooks since it is inherited by models.\n\n    def training_step(self,batch):\n        model.train()\n        concat_bert_indices,concat_segments_indices,text_bert_indices,aspect_bert_indices,labels=batch\n    #print(batch)\n    #print(text)\n    #out=bert(text_bert_indices)\n        out=self(concat_bert_indices,concat_segments_indices,text_bert_indices,aspect_bert_indices)\n        #labels=int(labels)\n        labels=labels.to(torch.long)\n        loss=criterion(out,labels)\n        #print('train')\n        #print(loss)\n        return loss\n    \n    def validation_step(self,batch):\n        model.eval()\n        concat_bert_indices,concat_segments_indices,text_bert_indices,aspect_bert_indices,labels=batch\n    #print(batch)\n    #print(text)\n    #out=bert(text_bert_indices)\n        out=self(concat_bert_indices,concat_segments_indices,text_bert_indices,aspect_bert_indices)\n        labels=labels.to(torch.long)\n        loss=criterion(out,labels)\n        acc=accuracy(labels,out)\n        #print('val')\n        #print(loss)\n        \n        return {'val_loss':loss.detach(),'val_acc':acc}\n    \n    def validation_epoch_end(self,result):\n        loss=[x['val_loss'] for x in result]\n        acc= [x['val_acc'] for x in result]\n        l_b=torch.stack(loss).mean()\n        a_b=torch.stack(acc).mean()\n        return {'val_loss':l_b.item(), 'val_acc': a_b.item()}\n    \n    def epoch_end(self,epoch,result):\n        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch,result['val_loss'], result['val_acc']))","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:14:29.11161Z","iopub.execute_input":"2022-01-04T17:14:29.111837Z","iopub.status.idle":"2022-01-04T17:14:29.121274Z","shell.execute_reply.started":"2022-01-04T17:14:29.111812Z","shell.execute_reply":"2022-01-04T17:14:29.120358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SelfAttention(AspectC):\n    def __init__(self, config, device,max_seq_len):\n        super(SelfAttention,self).__init__()\n        #self.opt = opt\n        self.max_seq_len=max_seq_len\n        self.device=device\n        self.config = config\n        self.SA = BertSelfAttention(config)\n        self.tanh = torch.nn.Tanh()\n\n    def forward(self, inputs):\n        zero_tensor = torch.tensor(np.zeros((inputs.size(0), 1, 1, self.max_seq_len),\n                                            dtype=np.float32), dtype=torch.float32).to(self.device)\n        SA_out = self.SA(inputs, zero_tensor)\n        return self.tanh(SA_out[0])\n\nclass CUST_BERT(AspectC):\n    def __init__(self, bert,dropout,bert_dim,device,max_seq_len,lcf):\n        super(CUST_BERT, self).__init__()\n\n        self.bert_spc = bert\n        self.max_seq_len=max_seq_len\n       # self.opt = opt\n        # self.bert_local = copy.deepcopy(bert)  # Uncomment the line to use dual Bert\n        self.bert_local = bert\n        self.local_context_focus=lcf\n        self.SRD=3\n        self.device=device\n        self.bert_dim=bert_dim# Default to use single Bert and reduce memory requirements\n        self.dropout = nn.Dropout(dropout)\n        self.bert_SA = SelfAttention(bert.config, device,self.max_seq_len)\n        self.linear_double = nn.Linear(self.bert_dim * 2, self.bert_dim)\n        self.linear_single = nn.Linear(self.bert_dim, self.bert_dim)\n       # self.BatchNorm=nn.BatchNorm1d(self.max_seq_len)\n        self.bert_pooler = BertPooler(bert.config)\n        self.dense = nn.Linear(self.bert_dim,3)\n        #self.softmax=nn.Softmax(dim=1)\n\n    def feature_dynamic_mask(self, text_local_indices, aspect_indices):\n        texts = text_local_indices.cpu().numpy()\n        asps = aspect_indices.cpu().numpy()\n        mask_len = self.SRD\n        masked_text_raw_indices = np.ones((text_local_indices.size(0), self.max_seq_len, self.bert_dim),\n                                          dtype=np.float32)\n        for text_i, asp_i in zip(range(len(texts)), range(len(asps))):\n            asp_len = np.count_nonzero(asps[asp_i]) - 2\n            try:\n                asp_begin = np.argwhere(texts[text_i] == asps[asp_i][1])[0][0]\n            except:\n                continue\n            if asp_begin >= mask_len:\n                mask_begin = asp_begin - mask_len\n            else:\n                mask_begin = 0\n            for i in range(mask_begin):\n                masked_text_raw_indices[text_i][i] = np.zeros((self.bert_dim), dtype=np.float)\n            for j in range(asp_begin + asp_len + mask_len, self.max_seq_len):\n                masked_text_raw_indices[text_i][j] = np.zeros((self.bert_dim), dtype=np.float)\n        masked_text_raw_indices = torch.from_numpy(masked_text_raw_indices)\n        return masked_text_raw_indices.to(self.device)\n\n    def feature_dynamic_weighted(self, text_local_indices, aspect_indices):\n        texts = text_local_indices.cpu().numpy()\n        asps = aspect_indices.cpu().numpy()\n        masked_text_raw_indices = np.ones((text_local_indices.size(0), self.max_seq_len, self.bert_dim),\n                                          dtype=np.float32)\n        for text_i, asp_i in zip(range(len(texts)), range(len(asps))):\n            asp_len = np.count_nonzero(asps[asp_i]) - 2\n            try:\n                asp_begin = np.argwhere(texts[text_i] == asps[asp_i][1])[0][0]\n                asp_avg_index = (asp_begin * 2 + asp_len) / 2\n            except:\n                continue\n            distances = np.zeros(np.count_nonzero(texts[text_i]), dtype=np.float32)\n            for i in range(1, np.count_nonzero(texts[text_i])-1):\n                if abs(i - asp_avg_index) + asp_len / 2 > self.SRD:\n                    distances[i] = 1 - (abs(i - asp_avg_index)+asp_len/2\n                                        - self.SRD)/np.count_nonzero(texts[text_i])\n                else:\n                    distances[i] = 1\n            for i in range(len(distances)):\n                masked_text_raw_indices[text_i][i] = masked_text_raw_indices[text_i][i] * distances[i]\n        masked_text_raw_indices = torch.from_numpy(masked_text_raw_indices)\n        return masked_text_raw_indices.to(self.device)\n\n    def forward(self,concat_bert_indices,concat_segments_indices,text_loc_indices,aspect_bert_indices):\n        text_bert_indices = concat_bert_indices\n        bert_segments_ids = concat_segments_indices\n        text_local_indices = text_loc_indices\n        aspect_indices = aspect_bert_indices\n\n        bert_spc_out= self.bert_spc(text_bert_indices, token_type_ids=bert_segments_ids)\n        #print(bert_spc_out)\n        bert_spc_out = self.dropout(bert_spc_out[0])\n\n        bert_local_out= self.bert_local(text_local_indices)\n        #print(bert_local_out)\n        #print(bert_spc_out)\n        bert_local_out = self.dropout(bert_local_out[0])\n\n        if self.local_context_focus == 'cdm':\n            masked_local_text_vec = self.feature_dynamic_mask(text_local_indices, aspect_indices)\n            bert_local_out = torch.mul(bert_local_out, masked_local_text_vec)\n\n        elif self.local_context_focus == 'cdw':\n            weighted_text_local_features = self.feature_dynamic_weighted(text_local_indices, aspect_indices)\n            bert_local_out = torch.mul(bert_local_out, weighted_text_local_features)\n\n        out_cat = torch.cat((bert_local_out, bert_spc_out), dim=-1)\n        mean_pool = self.linear_double(out_cat)\n        #mean_pool=self.BatchNorm(mean_pool)\n        self_attention_out = self.bert_SA(mean_pool)\n        pooled_out = self.bert_pooler(self_attention_out)\n        dense_out = self.dense(pooled_out)\n        #dense_out=self.softmax(dense_out)\n\n        return dense_out","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:17:58.10497Z","iopub.execute_input":"2022-01-04T17:17:58.10534Z","iopub.status.idle":"2022-01-04T17:17:58.139011Z","shell.execute_reply.started":"2022-01-04T17:17:58.105286Z","shell.execute_reply":"2022-01-04T17:17:58.138191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef pad_and_truncate(sequence, maxlen, dtype='int64', padding='post', truncating='post', value=0):\n    x = (np.ones(maxlen) * value).astype(dtype)\n    if truncating == 'pre':\n        trunc = sequence[-maxlen:]\n    else:\n        trunc = sequence[:maxlen]\n    trunc = np.asarray(trunc, dtype=dtype)\n    if padding == 'post':\n        x[:len(trunc)] = trunc\n    else:\n        x[-len(trunc):] = trunc\n    return x\n\n\nclass Tokenizer_Bert:\n    def __init__(self, max_seq_len, pretrained_bert_name):\n        self.tokenizer = BertTokenizer.from_pretrained(pretrained_bert_name)\n        self.max_seq_len = max_seq_len\n\n    def text_to_sequence(self, text, reverse=False, padding='post', truncating='post'):\n        sequence = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(text))\n        if len(sequence) == 0:\n            sequence = [0]\n        if reverse:\n            sequence = sequence[::-1]\n        return pad_and_truncate(sequence, self.max_seq_len, padding=padding, truncating=truncating)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:17:59.859277Z","iopub.execute_input":"2022-01-04T17:17:59.859822Z","iopub.status.idle":"2022-01-04T17:17:59.87073Z","shell.execute_reply.started":"2022-01-04T17:17:59.859784Z","shell.execute_reply":"2022-01-04T17:17:59.869962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer=Tokenizer_Bert(max_tex,pretrained_bert)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:14:31.029111Z","iopub.execute_input":"2022-01-04T17:14:31.02978Z","iopub.status.idle":"2022-01-04T17:14:34.432394Z","shell.execute_reply.started":"2022-01-04T17:14:31.029744Z","shell.execute_reply":"2022-01-04T17:14:34.431637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:14:34.434088Z","iopub.execute_input":"2022-01-04T17:14:34.434359Z","iopub.status.idle":"2022-01-04T17:14:34.439222Z","shell.execute_reply.started":"2022-01-04T17:14:34.434324Z","shell.execute_reply":"2022-01-04T17:14:34.438539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dataset_creator(text,aspect,text_len,aspect_len):\n    concat_bert_indices = tokenizer.text_to_sequence('[CLS] ' + text+ ' [SEP] ' + aspect + \" [SEP] \")\n    concat_segments_indices = [0] * (text_len + 2) + [1] * (aspect_len + 1)\n    concat_segments_indices = pad_and_truncate(concat_segments_indices, tokenizer.max_seq_len)\n\n    text_bert_indices = tokenizer.text_to_sequence(\"[CLS] \" + text + \" [SEP]\")\n    aspect_bert_indices = tokenizer.text_to_sequence(\"[CLS] \" + aspect + \" [SEP]\")\n    return concat_bert_indices,concat_segments_indices,text_bert_indices,aspect_bert_indices","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:14:34.440606Z","iopub.execute_input":"2022-01-04T17:14:34.441034Z","iopub.status.idle":"2022-01-04T17:14:34.448917Z","shell.execute_reply.started":"2022-01-04T17:14:34.440994Z","shell.execute_reply":"2022-01-04T17:14:34.448029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef final_dataset(data):\n    concat_bert_indices=np.empty([4000,max_tex])\n    concat_segments_indices=np.empty([4000,max_tex])\n    text_bert_indices=np.empty([4000,max_tex])\n    aspect_bert_indices=np.empty([4000,max_tex])\n    for i in range(len(data)):\n        sent=data.loc[i,'text']\n        asp=data.loc[i,'aspect']\n        \n        sent_len=len(data.loc[i,'text_tok'])\n        asp_len=len(data.loc[i,'aspect_tok'])\n        concat_bert_indices_1,concat_segments_indices_1,text_bert_indices_1,aspect_bert_indices_1=dataset_creator(sent,asp,sent_len,asp_len)\n        concat_bert_indices[i]=concat_bert_indices_1\n        concat_segments_indices[i]=concat_segments_indices_1\n        text_bert_indices[i]=text_bert_indices_1\n        aspect_bert_indices[i]=aspect_bert_indices_1\n        \n    return concat_bert_indices,concat_segments_indices,text_bert_indices,aspect_bert_indices\n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:14:34.45114Z","iopub.execute_input":"2022-01-04T17:14:34.451579Z","iopub.status.idle":"2022-01-04T17:14:34.459727Z","shell.execute_reply.started":"2022-01-04T17:14:34.45154Z","shell.execute_reply":"2022-01-04T17:14:34.458968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:14:34.461057Z","iopub.execute_input":"2022-01-04T17:14:34.461508Z","iopub.status.idle":"2022-01-04T17:14:34.48703Z","shell.execute_reply.started":"2022-01-04T17:14:34.461467Z","shell.execute_reply":"2022-01-04T17:14:34.486267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_bert_indices,concat_segments_indices,text_bert_indices,aspect_bert_indices=final_dataset(data)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:14:37.991853Z","iopub.execute_input":"2022-01-04T17:14:37.992131Z","iopub.status.idle":"2022-01-04T17:14:43.380754Z","shell.execute_reply.started":"2022-01-04T17:14:37.992103Z","shell.execute_reply":"2022-01-04T17:14:43.380021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_bert_indices","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:14:43.38236Z","iopub.execute_input":"2022-01-04T17:14:43.382602Z","iopub.status.idle":"2022-01-04T17:14:43.393396Z","shell.execute_reply.started":"2022-01-04T17:14:43.382568Z","shell.execute_reply":"2022-01-04T17:14:43.392484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_bert_indices.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:14:43.395066Z","iopub.execute_input":"2022-01-04T17:14:43.395358Z","iopub.status.idle":"2022-01-04T17:14:43.403275Z","shell.execute_reply.started":"2022-01-04T17:14:43.395316Z","shell.execute_reply":"2022-01-04T17:14:43.402302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating train and validation sets.Since data is almost at random\n,we can directly do splitting without using any sampler.","metadata":{}},{"cell_type":"code","source":"train_concat_bert_indices=torch.LongTensor(concat_bert_indices[:3200])\ntrain_concat_segments_indices=torch.LongTensor(concat_segments_indices[:3200])\ntrain_text_bert_indices=torch.LongTensor(text_bert_indices[:3200])\ntrain_aspect_bert_indices=torch.LongTensor(aspect_bert_indices[:3200])\ntrain_labels=torch.LongTensor(labels[:3200])\nval_concat_bert_indices=torch.LongTensor(concat_bert_indices[3200:])\nval_concat_segments_indices=torch.LongTensor(concat_segments_indices[3200:])\nval_text_bert_indices=torch.LongTensor(text_bert_indices[3200:])\nval_aspect_bert_indices=torch.LongTensor(aspect_bert_indices[3200:])\nval_labels=torch.LongTensor(labels[3200:])\n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:14:43.406349Z","iopub.execute_input":"2022-01-04T17:14:43.406905Z","iopub.status.idle":"2022-01-04T17:14:43.425598Z","shell.execute_reply.started":"2022-01-04T17:14:43.406863Z","shell.execute_reply":"2022-01-04T17:14:43.424866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(train_concat_bert_indices[0][0])","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:14:43.426968Z","iopub.execute_input":"2022-01-04T17:14:43.427258Z","iopub.status.idle":"2022-01-04T17:14:43.434768Z","shell.execute_reply.started":"2022-01-04T17:14:43.427219Z","shell.execute_reply":"2022-01-04T17:14:43.43378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds=TensorDataset(train_concat_bert_indices,train_concat_segments_indices,train_text_bert_indices,train_aspect_bert_indices,train_labels)\nbatch_size = 8\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=False)#creating dataloader for input to model","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:14:43.436566Z","iopub.execute_input":"2022-01-04T17:14:43.437554Z","iopub.status.idle":"2022-01-04T17:14:43.442856Z","shell.execute_reply.started":"2022-01-04T17:14:43.437505Z","shell.execute_reply":"2022-01-04T17:14:43.44207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_ds=TensorDataset(val_concat_bert_indices,val_concat_segments_indices,val_text_bert_indices,val_aspect_bert_indices,val_labels)\nbatch_size = 8\nval_dl = DataLoader(val_ds, batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:14:43.444586Z","iopub.execute_input":"2022-01-04T17:14:43.445675Z","iopub.status.idle":"2022-01-04T17:14:43.452591Z","shell.execute_reply.started":"2022-01-04T17:14:43.44559Z","shell.execute_reply":"2022-01-04T17:14:43.451662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dl=DeviceDataLoader(train_dl,device)#transporting dataloaders to device\nval_dl=DeviceDataLoader(val_dl,device)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:14:43.454819Z","iopub.execute_input":"2022-01-04T17:14:43.4556Z","iopub.status.idle":"2022-01-04T17:14:43.462928Z","shell.execute_reply.started":"2022-01-04T17:14:43.455554Z","shell.execute_reply":"2022-01-04T17:14:43.462092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#device","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:14:43.816685Z","iopub.execute_input":"2022-01-04T17:14:43.816886Z","iopub.status.idle":"2022-01-04T17:14:43.820625Z","shell.execute_reply.started":"2022-01-04T17:14:43.816864Z","shell.execute_reply":"2022-01-04T17:14:43.819752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert = BertModel.from_pretrained(pretrained_bert)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:14:45.694176Z","iopub.execute_input":"2022-01-04T17:14:45.694721Z","iopub.status.idle":"2022-01-04T17:14:48.313944Z","shell.execute_reply.started":"2022-01-04T17:14:45.694683Z","shell.execute_reply":"2022-01-04T17:14:48.3132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#bert","metadata":{"execution":{"iopub.status.busy":"2022-01-04T16:08:19.112846Z","iopub.execute_input":"2022-01-04T16:08:19.113405Z","iopub.status.idle":"2022-01-04T16:08:19.130118Z","shell.execute_reply.started":"2022-01-04T16:08:19.113368Z","shell.execute_reply":"2022-01-04T16:08:19.129412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for batch in train_dl:\n #   concat_bert_indices,concat_segments_indices,text_bert_indices,aspect_bert_indices,labels=batch\n  #  print(labels)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T16:08:19.133954Z","iopub.execute_input":"2022-01-04T16:08:19.135563Z","iopub.status.idle":"2022-01-04T16:08:19.140217Z","shell.execute_reply.started":"2022-01-04T16:08:19.135526Z","shell.execute_reply":"2022-01-04T16:08:19.139516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model =CUST_BERT(bert,0.1,768,device,max_tex,'cdw')#creating an instance of model class\n#model.double()\n#model\n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:18:15.710565Z","iopub.execute_input":"2022-01-04T17:18:15.711038Z","iopub.status.idle":"2022-01-04T17:18:15.746696Z","shell.execute_reply.started":"2022-01-04T17:18:15.710999Z","shell.execute_reply":"2022-01-04T17:18:15.746009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_device(model, device)#transporting model to GPU","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:18:16.746097Z","iopub.execute_input":"2022-01-04T17:18:16.74637Z","iopub.status.idle":"2022-01-04T17:18:16.768961Z","shell.execute_reply.started":"2022-01-04T17:18:16.746339Z","shell.execute_reply":"2022-01-04T17:18:16.768234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy(act,pred):\n    _, preds = torch.max(pred, dim=1)\n    print(preds)\n    #print(preds[0])\n    #print(act[0])\n    return torch.tensor(torch.sum(preds==act).item() / len(preds))","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:18:24.535723Z","iopub.execute_input":"2022-01-04T17:18:24.53599Z","iopub.status.idle":"2022-01-04T17:18:24.541139Z","shell.execute_reply.started":"2022-01-04T17:18:24.535962Z","shell.execute_reply":"2022-01-04T17:18:24.54019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, val_loader):\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:18:24.971511Z","iopub.execute_input":"2022-01-04T17:18:24.972357Z","iopub.status.idle":"2022-01-04T17:18:24.977157Z","shell.execute_reply.started":"2022-01-04T17:18:24.972313Z","shell.execute_reply":"2022-01-04T17:18:24.97636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.Adam):\n    optimizer = opt_func(model.parameters(), lr)\n    history = [] # for recording epoch-wise results\n    \n    for epoch in range(epochs):\n        \n        # Training Phase \n        for batch in train_loader:\n            loss = model.training_step(batch)\n            \n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        #print(loss)\n        \n        # Validation phase\n        with torch.no_grad():\n            result = evaluate(model, val_loader)\n        model.epoch_end(epoch, result)\n    \n        history.append(result)\n        \n\n    return history","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:18:27.141151Z","iopub.execute_input":"2022-01-04T17:18:27.141498Z","iopub.status.idle":"2022-01-04T17:18:27.14996Z","shell.execute_reply.started":"2022-01-04T17:18:27.141461Z","shell.execute_reply":"2022-01-04T17:18:27.148754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fit(5, 2*10e-6, model, train_dl, val_dl)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:18:28.51544Z","iopub.execute_input":"2022-01-04T17:18:28.515718Z","iopub.status.idle":"2022-01-04T17:35:01.860323Z","shell.execute_reply.started":"2022-01-04T17:18:28.515691Z","shell.execute_reply":"2022-01-04T17:35:01.859483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fit(5, 5*10e-7, model, train_dl, val_dl)  ","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:40:07.528906Z","iopub.execute_input":"2022-01-04T17:40:07.529455Z","iopub.status.idle":"2022-01-04T17:56:40.420844Z","shell.execute_reply.started":"2022-01-04T17:40:07.529414Z","shell.execute_reply":"2022-01-04T17:56:40.420118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'aoa_bert_10epochs_cdw_2e6_nbrweights.pth')#saving model state dict","metadata":{"execution":{"iopub.status.busy":"2022-01-04T18:05:08.277473Z","iopub.execute_input":"2022-01-04T18:05:08.278227Z","iopub.status.idle":"2022-01-04T18:05:08.975963Z","shell.execute_reply.started":"2022-01-04T18:05:08.278174Z","shell.execute_reply":"2022-01-04T18:05:08.975241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model, 'aoa_bert_10epochs_cdw_2e6_dropout_nobatch_entire_model_nbr.pth')#saving entire model","metadata":{"execution":{"iopub.status.busy":"2022-01-04T18:05:09.793579Z","iopub.execute_input":"2022-01-04T18:05:09.794486Z","iopub.status.idle":"2022-01-04T18:05:10.48306Z","shell.execute_reply.started":"2022-01-04T18:05:09.79443Z","shell.execute_reply":"2022-01-04T18:05:10.482335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}