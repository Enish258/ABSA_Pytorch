{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a8c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import os\n",
    "import urllib.request\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import os\n",
    "import torchvision\n",
    "import tarfile\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.datasets.utils import download_url\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import spacy\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd6f6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel\n",
    "\n",
    "from transformers.models.bert.modeling_bert import BertPooler, BertSelfAttention\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "pretrained_bert=\"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ba43d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class aspectClassificationBase(nn.Module):\n",
    "\n",
    "    def training_step(self,batch):#batch wise training\n",
    "        model.train()\n",
    "        text,aspect,labels=batch\n",
    "        out=self(text,aspect)\n",
    "        #labels=int(labels)\n",
    "        labels=labels.to(torch.long)\n",
    "        loss=F.cross_entropy(out,labels)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self,batch):#batch wise validation\n",
    "        model.eval()\n",
    "        text,aspect,labels=batch\n",
    "        out=self(text,aspect)\n",
    "        labels=labels.to(torch.long)\n",
    "        loss=F.cross_entropy(out,labels)\n",
    "        acc=accuracy(labels,out)\n",
    "        return {'val_loss':loss.detach(),'val_acc':acc}\n",
    "    \n",
    "    def validation_epoch_end(self,result):\n",
    "        loss=[x['val_loss'] for x in result]\n",
    "        acc= [x['val_acc'] for x in result]\n",
    "        l_b=torch.stack(loss).mean()\n",
    "        a_b=torch.stack(acc).mean()\n",
    "        return {'val_loss':l_b.item(), 'val_acc': a_b.item()}\n",
    "    \n",
    "    def epoch_end(self,epoch,result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch,result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c36ae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_AOA(aspectClassificationBase):#model class for Attention over attention\n",
    "    def __init__(self,emb_mat,num_classes,embedding_dim,vocab_size_t,vocab_size_a):#emb_mat stores glove embeddings of all \n",
    "                                                                                   #words in our dictionary\n",
    "        super(LSTM_AOA,self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size_t = vocab_size_t\n",
    "        self.vocab_size_a=vocab_size_a\n",
    "        self.polarities=num_classes\n",
    "        self.embed = nn.Embedding.from_pretrained(emb_mat,freeze=True)#use glove word embedding matrix \n",
    "        self.lstm =nn.LSTM(self.embedding_dim,128,batch_first=True,num_layers=1,bidirectional=True)\n",
    "        #self.lstm_a  =nn.LSTM(self.embedding_dim,128,batch_first=True,bidirectional=True)\n",
    "        self.dense = nn.Linear(256,self.polarities)\n",
    "        self.softmax_c=nn.Softmax(dim=2)\n",
    "        self.softmax_r=nn.Softmax(dim=1)\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, text,aspect,batch_size):\n",
    "        #asp_raw_indices=inputs[:,278:,:]\n",
    "       # y1=inputs[0][278:][:]\n",
    "        #y2=inputs[1][278:][:]\n",
    "        \n",
    "        #text_raw_indices = torch.stack([x1,x2])\n",
    "        #asp_raw_indices=torch.stack([y1,y2])\n",
    "        #print(asp_raw_indices.shape)\n",
    "        #print(text_raw_indices.shape)\n",
    "        x= self.embed(text)\n",
    "        x,_=self.lstm(x)\n",
    "       # print(x.shape)\n",
    "        \n",
    "        x2=self.embed(aspect)\n",
    "        x2,_=self.lstm(x2)\n",
    "        #print(x2.shape)\n",
    "        x2=x2.permute(0,2,1)\n",
    "        \n",
    "        x3=torch.matmul(x,x2)\n",
    "        #print(x3.shape)\n",
    "        x4=x3.detach().clone()\n",
    "        x5=x3.detach().clone()\n",
    "        x5=self.softmax_c(x5)\n",
    "        #print(x5.shape)\n",
    "        x4=self.softmax_r(x4)\n",
    "        #x4=x4.permute(0,2,1)\n",
    "        x5=x5.sum(dim=1)/self.vocab_size_t\n",
    "        #print(x5.shape)\n",
    "        #print(x5)\n",
    "        x5=x5.reshape(batch_size,self.vocab_size_a,1)\n",
    "        x6=torch.matmul(x4,x5)\n",
    "     \n",
    "       # print(x6.shape)\n",
    "       # print(x6.shape)\n",
    "        x=x.permute(0,2,1)\n",
    "       # print(x.shape)\n",
    "        x7=torch.matmul(x,x6)\n",
    "        #print(x7.shape)\n",
    "        \n",
    "        \n",
    "        x7=x7.reshape(batch_size,-1)\n",
    "        #print(x7.shape)\n",
    "        x7=self.dense(x7)#no need for softmax if cross entropy loss is being used as a loss function.\n",
    "        #print(x7.shape)\n",
    "        #out=self.softmax(x7)\n",
    "        #x_len = torch.sum(text_raw_indices != 0, dim=-1)\n",
    "        #_, (h_n, _) = self.lstm(x, x_len)\n",
    "        #out = self.dense(h_n[0])\n",
    "        return x7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
