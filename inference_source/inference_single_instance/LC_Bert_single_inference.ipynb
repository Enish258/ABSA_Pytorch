{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T20:21:27.697063Z",
     "iopub.status.busy": "2022-01-05T20:21:27.696074Z",
     "iopub.status.idle": "2022-01-05T20:21:40.870077Z",
     "shell.execute_reply": "2022-01-05T20:21:40.868901Z",
     "shell.execute_reply.started": "2022-01-05T20:21:27.696926Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import os\n",
    "import urllib.request\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import os\n",
    "import torchvision\n",
    "import tarfile\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.datasets.utils import download_url\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import spacy\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T20:21:40.872778Z",
     "iopub.status.busy": "2022-01-05T20:21:40.872433Z",
     "iopub.status.idle": "2022-01-05T20:21:42.239787Z",
     "shell.execute_reply": "2022-01-05T20:21:42.238656Z",
     "shell.execute_reply.started": "2022-01-05T20:21:40.872743Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel\n",
    "\n",
    "from transformers.models.bert.modeling_bert import BertPooler, BertSelfAttention\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "pretrained_bert=\"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T20:21:42.241798Z",
     "iopub.status.busy": "2022-01-05T20:21:42.241449Z",
     "iopub.status.idle": "2022-01-05T20:21:42.256342Z",
     "shell.execute_reply": "2022-01-05T20:21:42.255520Z",
     "shell.execute_reply.started": "2022-01-05T20:21:42.241756Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#for getting the package versions used in this notebook\n",
    "\n",
    "import pkg_resources\n",
    "import types\n",
    "def get_imports():\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            # Split ensures you get root package, \n",
    "            # not just imported function\n",
    "            name = val.__name__.split(\".\")[0]\n",
    "\n",
    "        elif isinstance(val, type):\n",
    "            name = val.__module__.split(\".\")[0]\n",
    "\n",
    "        # Some packages are weird and have different\n",
    "        # imported names vs. system names\n",
    "        if name == \"PIL\":\n",
    "            name = \"Pillow\"\n",
    "        elif name == \"sklearn\":\n",
    "            name = \"scikit-learn\"\n",
    "\n",
    "        yield name\n",
    "imports = list(set(get_imports()))\n",
    "\n",
    "requirements = []\n",
    "for m in pkg_resources.working_set:\n",
    "    if m.project_name in imports and m.project_name!=\"pip\":\n",
    "        requirements.append((m.project_name, m.version))\n",
    "\n",
    "for r in requirements:\n",
    "    print(\"{}=={}\".format(*r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T20:26:10.907035Z",
     "iopub.status.busy": "2022-01-05T20:26:10.906616Z",
     "iopub.status.idle": "2022-01-05T20:26:10.917793Z",
     "shell.execute_reply": "2022-01-05T20:26:10.916803Z",
     "shell.execute_reply.started": "2022-01-05T20:26:10.906987Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_and_truncate(sequence, maxlen, dtype='int64', padding='post', truncating='post', value=0):\n",
    "    x = (np.ones(maxlen) * value).astype(dtype)\n",
    "    if truncating == 'pre':\n",
    "        trunc = sequence[-maxlen:]\n",
    "    else:\n",
    "        trunc = sequence[:maxlen]\n",
    "    trunc = np.asarray(trunc, dtype=dtype)\n",
    "    if padding == 'post':\n",
    "        x[:len(trunc)] = trunc\n",
    "    else:\n",
    "        x[-len(trunc):] = trunc\n",
    "    return x\n",
    "\n",
    "\n",
    "class Tokenizer_Bert:\n",
    "    def __init__(self, max_seq_len, pretrained_bert_name):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_bert_name)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def text_to_sequence(self, text, reverse=False, padding='post', truncating='post'):\n",
    "        sequence = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(text))\n",
    "        if len(sequence) == 0:\n",
    "            sequence = [0]\n",
    "        if reverse:\n",
    "            sequence = sequence[::-1]\n",
    "        return pad_and_truncate(sequence, self.max_seq_len, padding=padding, truncating=truncating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T20:21:42.263640Z",
     "iopub.status.busy": "2022-01-05T20:21:42.258624Z",
     "iopub.status.idle": "2022-01-05T20:21:42.277111Z",
     "shell.execute_reply": "2022-01-05T20:21:42.276176Z",
     "shell.execute_reply.started": "2022-01-05T20:21:42.263593Z"
    }
   },
   "outputs": [],
   "source": [
    "#function to get the device being used.use CUDA for inferencing\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T20:21:42.279673Z",
     "iopub.status.busy": "2022-01-05T20:21:42.278773Z",
     "iopub.status.idle": "2022-01-05T20:21:42.293737Z",
     "shell.execute_reply": "2022-01-05T20:21:42.292487Z",
     "shell.execute_reply.started": "2022-01-05T20:21:42.279627Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#utility class for training and testing.Needs to be present in all notebooks since it is inherited by models.\n",
    "class AspectC(nn.Module):\n",
    "\n",
    "    def training_step(self,batch):\n",
    "        model.train()\n",
    "        concat_bert_indices,concat_segments_indices,text_bert_indices,aspect_bert_indices,labels=batch\n",
    "    #print(batch)\n",
    "    #print(text)\n",
    "    #out=bert(text_bert_indices)\n",
    "        out=self(concat_bert_indices,concat_segments_indices,text_bert_indices,aspect_bert_indices)\n",
    "        #labels=int(labels)\n",
    "        labels=labels.to(torch.long)\n",
    "        loss=criterion(out,labels)\n",
    "        #print('train')\n",
    "        #print(loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self,batch):\n",
    "        model.eval()\n",
    "        concat_bert_indices,concat_segments_indices,text_bert_indices,aspect_bert_indices,labels=batch\n",
    "    #print(batch)\n",
    "    #print(text)\n",
    "    #out=bert(text_bert_indices)\n",
    "        out=self(concat_bert_indices,concat_segments_indices,text_bert_indices,aspect_bert_indices)\n",
    "        labels=labels.to(torch.long)\n",
    "        loss=criterion(out,labels)\n",
    "        acc=accuracy(labels,out)\n",
    "        #print('val')\n",
    "        #print(loss)\n",
    "        \n",
    "        return {'val_loss':loss.detach(),'val_acc':acc}\n",
    "    \n",
    "    def validation_epoch_end(self,result):\n",
    "        loss=[x['val_loss'] for x in result]\n",
    "        acc= [x['val_acc'] for x in result]\n",
    "        l_b=torch.stack(loss).mean()\n",
    "        a_b=torch.stack(acc).mean()\n",
    "        return {'val_loss':l_b.item(), 'val_acc': a_b.item()}\n",
    "    \n",
    "    def epoch_end(self,epoch,result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch,result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T20:21:42.298015Z",
     "iopub.status.busy": "2022-01-05T20:21:42.297741Z",
     "iopub.status.idle": "2022-01-05T20:21:42.590396Z",
     "shell.execute_reply": "2022-01-05T20:21:42.589287Z",
     "shell.execute_reply.started": "2022-01-05T20:21:42.297986Z"
    }
   },
   "outputs": [],
   "source": [
    "class SelfAttention(AspectC):#implementsa custom self attention layer\n",
    "    def __init__(self, config, device,max_seq_len):\n",
    "        super(SelfAttention,self).__init__()\n",
    "        #self.opt = opt\n",
    "        self.max_seq_len=max_seq_len\n",
    "        self.device=device\n",
    "        self.config = config\n",
    "        self.SA = BertSelfAttention(config)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        zero_tensor = torch.tensor(np.zeros((inputs.size(0), 1, 1, self.max_seq_len),\n",
    "                                            dtype=np.float32), dtype=torch.float32).to(self.device)\n",
    "        SA_out = self.SA(inputs, zero_tensor)\n",
    "        return self.tanh(SA_out[0])\n",
    "    \n",
    "class CUST_BERT(AspectC):\n",
    "    def __init__(self, bert,dropout,bert_dim,device,max_seq_len,lcf):\n",
    "        super(CUST_BERT, self).__init__()\n",
    "\n",
    "        self.bert_spc = bert\n",
    "        self.max_seq_len=max_seq_len\n",
    "       # self.opt = opt\n",
    "        # self.bert_local = copy.deepcopy(bert)  # Uncomment the line to use dual Bert\n",
    "        self.bert_local = bert\n",
    "        self.local_context_focus=lcf\n",
    "        self.SRD=3\n",
    "        self.device=device\n",
    "        self.bert_dim=bert_dim# Default to use single Bert and reduce memory requirements\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bert_SA = SelfAttention(bert.config, device,self.max_seq_len)\n",
    "        self.linear_double = nn.Linear(self.bert_dim * 2, self.bert_dim)\n",
    "        self.linear_single = nn.Linear(self.bert_dim, self.bert_dim)\n",
    "       # self.BatchNorm=nn.BatchNorm1d(self.max_seq_len)\n",
    "        self.bert_pooler = BertPooler(bert.config)\n",
    "        self.dense = nn.Linear(self.bert_dim,3)\n",
    "        #self.softmax=nn.Softmax(dim=1)\n",
    "\n",
    " \n",
    "    def feature_dynamic_mask(self, text_local_indices, aspect_indices):\n",
    "        texts = text_local_indices.cpu().numpy()\n",
    "        asps = aspect_indices.cpu().numpy()\n",
    "        mask_len = self.SRD\n",
    "        masked_text_raw_indices = np.ones((text_local_indices.size(0), self.max_seq_len, self.bert_dim),\n",
    "                                          dtype=np.float32)\n",
    "        for text_i, asp_i in zip(range(len(texts)), range(len(asps))):\n",
    "            asp_len = np.count_nonzero(asps[asp_i]) - 2\n",
    "            try:\n",
    "                asp_begin = np.argwhere(texts[text_i] == asps[asp_i][1])[0][0]\n",
    "            except:\n",
    "                continue\n",
    "            if asp_begin >= mask_len:\n",
    "                mask_begin = asp_begin - mask_len\n",
    "            else:\n",
    "                mask_begin = 0\n",
    "            for i in range(mask_begin):\n",
    "                masked_text_raw_indices[text_i][i] = np.zeros((self.bert_dim), dtype=np.float)\n",
    "            for j in range(asp_begin + asp_len + mask_len, self.max_seq_len):\n",
    "                masked_text_raw_indices[text_i][j] = np.zeros((self.bert_dim), dtype=np.float)\n",
    "        masked_text_raw_indices = torch.from_numpy(masked_text_raw_indices)\n",
    "        return masked_text_raw_indices.to(self.device)\n",
    "    \n",
    "    def feature_dynamic_weighted(self, text_local_indices, aspect_indices):\n",
    "        texts = text_local_indices.cpu().numpy()\n",
    "        asps = aspect_indices.cpu().numpy()\n",
    "        masked_text_raw_indices = np.ones((text_local_indices.size(0), self.max_seq_len, self.bert_dim),\n",
    "                                          dtype=np.float32)\n",
    "        for text_i, asp_i in zip(range(len(texts)), range(len(asps))):\n",
    "            asp_len = np.count_nonzero(asps[asp_i]) - 2\n",
    "            try:\n",
    "                asp_begin = np.argwhere(texts[text_i] == asps[asp_i][1])[0][0]\n",
    "                asp_avg_index = (asp_begin * 2 + asp_len) / 2\n",
    "            except:\n",
    "                continue\n",
    "            distances = np.zeros(np.count_nonzero(texts[text_i]), dtype=np.float32)\n",
    "            for i in range(1, np.count_nonzero(texts[text_i])-1):\n",
    "                if abs(i - asp_avg_index) + asp_len / 2 > self.SRD:\n",
    "                    distances[i] = 1 - (abs(i - asp_avg_index)+asp_len/2\n",
    "                                        - self.SRD)/np.count_nonzero(texts[text_i])\n",
    "                else:\n",
    "                    distances[i] = 1\n",
    "            for i in range(len(distances)):\n",
    "                masked_text_raw_indices[text_i][i] = masked_text_raw_indices[text_i][i] * distances[i]\n",
    "        masked_text_raw_indices = torch.from_numpy(masked_text_raw_indices)\n",
    "        return masked_text_raw_indices.to(self.device)\n",
    "    \n",
    "    \n",
    "    def forward(self,concat_bert_indices,concat_segments_indices,text_loc_indices,aspect_bert_indices):\n",
    "        text_bert_indices = concat_bert_indices\n",
    "        bert_segments_ids = concat_segments_indices\n",
    "        text_local_indices = text_loc_indices\n",
    "        aspect_indices = aspect_bert_indices\n",
    "\n",
    "        bert_spc_out= self.bert_spc(text_bert_indices, token_type_ids=bert_segments_ids)\n",
    "        #print(bert_spc_out)\n",
    "        bert_spc_out = self.dropout(bert_spc_out[0])\n",
    "\n",
    "        bert_local_out= self.bert_local(text_local_indices)\n",
    "        #print(bert_local_out)\n",
    "        #print(bert_spc_out)\n",
    "        bert_local_out = self.dropout(bert_local_out[0])\n",
    "\n",
    "        if self.local_context_focus == 'cdm':\n",
    "            masked_local_text_vec = self.feature_dynamic_mask(text_local_indices, aspect_indices)\n",
    "            bert_local_out = torch.mul(bert_local_out, masked_local_text_vec)\n",
    "\n",
    "        elif self.local_context_focus == 'cdw':\n",
    "            weighted_text_local_features = self.feature_dynamic_weighted(text_local_indices, aspect_indices)\n",
    "            bert_local_out = torch.mul(bert_local_out, weighted_text_local_features)\n",
    "\n",
    "        out_cat = torch.cat((bert_local_out, bert_spc_out), dim=-1)\n",
    "        mean_pool = self.linear_double(out_cat)\n",
    "        #mean_pool=self.BatchNorm(mean_pool)\n",
    "        self_attention_out = self.bert_SA(mean_pool)\n",
    "        pooled_out = self.bert_pooler(self_attention_out)\n",
    "        dense_out = self.dense(pooled_out)\n",
    "        #dense_out=self.softmax(dense_out)\n",
    "\n",
    "        return dense_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T20:21:42.593710Z",
     "iopub.status.busy": "2022-01-05T20:21:42.592205Z",
     "iopub.status.idle": "2022-01-05T20:21:42.605481Z",
     "shell.execute_reply": "2022-01-05T20:21:42.604432Z",
     "shell.execute_reply.started": "2022-01-05T20:21:42.593658Z"
    }
   },
   "outputs": [],
   "source": [
    "device=get_default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T20:41:18.174041Z",
     "iopub.status.busy": "2022-01-05T20:41:18.173718Z",
     "iopub.status.idle": "2022-01-05T20:41:34.841773Z",
     "shell.execute_reply": "2022-01-05T20:41:34.840882Z",
     "shell.execute_reply.started": "2022-01-05T20:41:18.174009Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T20:41:39.220506Z",
     "iopub.status.busy": "2022-01-05T20:41:39.220130Z",
     "iopub.status.idle": "2022-01-05T20:41:41.522417Z",
     "shell.execute_reply": "2022-01-05T20:41:41.521258Z",
     "shell.execute_reply.started": "2022-01-05T20:41:39.220472Z"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T20:31:55.626405Z",
     "iopub.status.busy": "2022-01-05T20:31:55.626115Z",
     "iopub.status.idle": "2022-01-05T20:31:58.308733Z",
     "shell.execute_reply": "2022-01-05T20:31:58.306884Z",
     "shell.execute_reply.started": "2022-01-05T20:31:55.626365Z"
    }
   },
   "outputs": [],
   "source": [
    "#available models are cdw and cdm.enter either of those\n",
    "model_mode= input(\"Enter model_mode\")\n",
    "print(model_mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T20:39:59.097601Z",
     "iopub.status.busy": "2022-01-05T20:39:59.097280Z",
     "iopub.status.idle": "2022-01-05T20:40:04.436438Z",
     "shell.execute_reply": "2022-01-05T20:40:04.435500Z",
     "shell.execute_reply.started": "2022-01-05T20:39:59.097567Z"
    }
   },
   "outputs": [],
   "source": [
    "model=torch.load('../input/aoa-bert-10-epochs-cdw-2e6-nobatch-models/aoa_bert_10epochs_cdw_2e6_dropout_nobatch_entire_model_nbr.pth')\n",
    "to_device(model,device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T20:43:40.636595Z",
     "iopub.status.busy": "2022-01-05T20:43:40.636279Z",
     "iopub.status.idle": "2022-01-05T20:43:40.644428Z",
     "shell.execute_reply": "2022-01-05T20:43:40.643139Z",
     "shell.execute_reply.started": "2022-01-05T20:43:40.636562Z"
    }
   },
   "outputs": [],
   "source": [
    "#function in which model inference occurs\n",
    "def infer(test_dl,model,b):\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dl:\n",
    "            concat_bert_indices,concat_segments_indices,text_bert_indices,aspect_bert_indices=batch\n",
    "            out=model(concat_bert_indices,concat_segments_indices,text_bert_indices,aspect_bert_indices)\n",
    "            #print(out)\n",
    "            \n",
    "            _, preds = torch.max(out, dim=1)\n",
    "            #print(preds)\n",
    "            preds=preds.cpu().detach().numpy()\n",
    "            preds=preds.tolist()\n",
    "            return {'label':preds[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T20:43:40.892383Z",
     "iopub.status.busy": "2022-01-05T20:43:40.891543Z",
     "iopub.status.idle": "2022-01-05T20:43:40.906319Z",
     "shell.execute_reply": "2022-01-05T20:43:40.905054Z",
     "shell.execute_reply.started": "2022-01-05T20:43:40.892345Z"
    }
   },
   "outputs": [],
   "source": [
    "#utility function\n",
    "def inf_prep(text,aspect,b,device):\n",
    "    if type(aspect)==float:\n",
    "        aspect=str(aspect)\n",
    "    text=text.lower()\n",
    "    split_ = WordPunctTokenizer()\n",
    "    text_tok= split_.tokenize(text)\n",
    "    text_len=len(text_tok)\n",
    "    aspect=aspect.lower()\n",
    "    aspect_tok = split_.tokenize(aspect)\n",
    "    aspect_len=len(aspect_tok)\n",
    "    max_tex=278\n",
    "    max_asp=8\n",
    "    tokenizer=Tokenizer_Bert(max_tex,pretrained_bert)\n",
    "    concat_bert_indices_1 = tokenizer.text_to_sequence('[CLS] ' + text+ ' [SEP] ' + aspect + \" [SEP] \")\n",
    "    concat_segments_indices_1 = [0] * (text_len + 2) + [1] * (aspect_len + 1)\n",
    "    concat_segments_indices_1 = pad_and_truncate(concat_segments_indices_1, tokenizer.max_seq_len)\n",
    "\n",
    "    text_bert_indices_1 = tokenizer.text_to_sequence(\"[CLS] \" + text + \" [SEP]\")\n",
    "    aspect_bert_indices_1 = tokenizer.text_to_sequence(\"[CLS] \" + aspect + \" [SEP]\")\n",
    "    concat_bert_indices=np.empty([b,max_tex])\n",
    "    concat_segments_indices=np.empty([b,max_tex])\n",
    "    text_bert_indices=np.empty([b,max_tex])\n",
    "    aspect_bert_indices=np.empty([b,max_tex])\n",
    "    for i in range(b):\n",
    "        concat_bert_indices[i]=concat_bert_indices_1\n",
    "        concat_segments_indices[i]=concat_segments_indices_1\n",
    "        text_bert_indices[i]=text_bert_indices_1\n",
    "        aspect_bert_indices[i]=aspect_bert_indices_1\n",
    "    \n",
    "    concat_bert_indices=torch.LongTensor(concat_bert_indices)\n",
    "    concat_segments_indices=torch.LongTensor(concat_segments_indices)\n",
    "    text_bert_indices=torch.LongTensor(text_bert_indices)\n",
    "    aspect_bert_indices=torch.LongTensor(aspect_bert_indices)\n",
    "    test_ds=TensorDataset(concat_bert_indices,concat_segments_indices,text_bert_indices,aspect_bert_indices)\n",
    "        \n",
    "    \n",
    "    test_dl = DataLoader(test_ds, b, shuffle=False)\n",
    "    test_dl=DeviceDataLoader(test_dl,device)\n",
    "    ans=infer(test_dl,model,b)\n",
    "    if ans['label']==0:\n",
    "        print('Negative sentiment')\n",
    "    elif ans['label']==1:\n",
    "        print('Neutral sentiment')\n",
    "    elif ans['label']==2:\n",
    "        print('Positive sentiment')\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T20:45:09.387923Z",
     "iopub.status.busy": "2022-01-05T20:45:09.387168Z",
     "iopub.status.idle": "2022-01-05T20:45:35.421000Z",
     "shell.execute_reply": "2022-01-05T20:45:35.419253Z",
     "shell.execute_reply.started": "2022-01-05T20:45:09.387835Z"
    }
   },
   "outputs": [],
   "source": [
    "text = input(\"Enter your text/context: \")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T20:45:37.690093Z",
     "iopub.status.busy": "2022-01-05T20:45:37.689732Z",
     "iopub.status.idle": "2022-01-05T20:45:40.230205Z",
     "shell.execute_reply": "2022-01-05T20:45:40.228398Z",
     "shell.execute_reply.started": "2022-01-05T20:45:37.690048Z"
    }
   },
   "outputs": [],
   "source": [
    "aspect= input(\"Enter aspect \")\n",
    "print(aspect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T20:45:42.593595Z",
     "iopub.status.busy": "2022-01-05T20:45:42.592478Z",
     "iopub.status.idle": "2022-01-05T20:45:45.904153Z",
     "shell.execute_reply": "2022-01-05T20:45:45.903101Z",
     "shell.execute_reply.started": "2022-01-05T20:45:42.593538Z"
    }
   },
   "outputs": [],
   "source": [
    "inf_prep(text,aspect,8,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
