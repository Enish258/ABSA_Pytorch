{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T18:45:58.697821Z",
     "iopub.status.busy": "2022-01-05T18:45:58.697497Z",
     "iopub.status.idle": "2022-01-05T18:46:09.010878Z",
     "shell.execute_reply": "2022-01-05T18:46:09.010089Z",
     "shell.execute_reply.started": "2022-01-05T18:45:58.697738Z"
    }
   },
   "source": [
    " Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import os\n",
    "import urllib.request\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import os\n",
    "import torchvision\n",
    "import tarfile\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.datasets.utils import download_url\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import spacy\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T18:46:09.012956Z",
     "iopub.status.busy": "2022-01-05T18:46:09.012693Z",
     "iopub.status.idle": "2022-01-05T18:46:10.105084Z",
     "shell.execute_reply": "2022-01-05T18:46:10.104329Z",
     "shell.execute_reply.started": "2022-01-05T18:46:09.012905Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel\n",
    "\n",
    "from transformers.models.bert.modeling_bert import BertPooler, BertSelfAttention\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "#pretrained_bert=\"bert-base-uncased\"\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T18:46:10.106645Z",
     "iopub.status.busy": "2022-01-05T18:46:10.106374Z",
     "iopub.status.idle": "2022-01-05T18:46:10.121133Z",
     "shell.execute_reply": "2022-01-05T18:46:10.120372Z",
     "shell.execute_reply.started": "2022-01-05T18:46:10.106594Z"
    }
   },
   "outputs": [],
   "source": [
    "#for getting the package versions used in this notebook\n",
    "\n",
    "import pkg_resources\n",
    "import types\n",
    "def get_imports():\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            # Split ensures you get root package, \n",
    "            # not just imported function\n",
    "            name = val.__name__.split(\".\")[0]\n",
    "\n",
    "        elif isinstance(val, type):\n",
    "            name = val.__module__.split(\".\")[0]\n",
    "\n",
    "        # Some packages are weird and have different\n",
    "        # imported names vs. system names\n",
    "        if name == \"PIL\":\n",
    "            name = \"Pillow\"\n",
    "        elif name == \"sklearn\":\n",
    "            name = \"scikit-learn\"\n",
    "\n",
    "        yield name\n",
    "imports = list(set(get_imports()))\n",
    "\n",
    "requirements = []\n",
    "for m in pkg_resources.working_set:\n",
    "    if m.project_name in imports and m.project_name!=\"pip\":\n",
    "        requirements.append((m.project_name, m.version))\n",
    "\n",
    "for r in requirements:\n",
    "    print(\"{}=={}\".format(*r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T18:46:10.123631Z",
     "iopub.status.busy": "2022-01-05T18:46:10.123238Z",
     "iopub.status.idle": "2022-01-05T18:46:10.136382Z",
     "shell.execute_reply": "2022-01-05T18:46:10.135541Z",
     "shell.execute_reply.started": "2022-01-05T18:46:10.123591Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#function to get the device being used.use CUDA for inferencing\n",
    "\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T18:46:10.139527Z",
     "iopub.status.busy": "2022-01-05T18:46:10.139091Z",
     "iopub.status.idle": "2022-01-05T18:46:10.150032Z",
     "shell.execute_reply": "2022-01-05T18:46:10.149236Z",
     "shell.execute_reply.started": "2022-01-05T18:46:10.139485Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#utility class for training and testing.Needs to be present in all notebooks since it is inherited by models.\n",
    "\n",
    "\n",
    "class aspectClassificationBase(nn.Module):\n",
    "\n",
    "    def training_step(self,batch):#batch wise training\n",
    "        model.train()\n",
    "        text,aspect,labels=batch\n",
    "        out=self(text,aspect)\n",
    "        #labels=int(labels)\n",
    "        labels=labels.to(torch.long)\n",
    "        loss=F.cross_entropy(out,labels)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self,batch):#batch wise validation\n",
    "        model.eval()\n",
    "        text,aspect,labels=batch\n",
    "        out=self(text,aspect)\n",
    "        labels=labels.to(torch.long)\n",
    "        loss=F.cross_entropy(out,labels)\n",
    "        acc=accuracy(labels,out)\n",
    "        return {'val_loss':loss.detach(),'val_acc':acc}\n",
    "    \n",
    "    def validation_epoch_end(self,result):\n",
    "        loss=[x['val_loss'] for x in result]\n",
    "        acc= [x['val_acc'] for x in result]\n",
    "        l_b=torch.stack(loss).mean()\n",
    "        a_b=torch.stack(acc).mean()\n",
    "        return {'val_loss':l_b.item(), 'val_acc': a_b.item()}\n",
    "    \n",
    "    def epoch_end(self,epoch,result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch,result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T18:46:10.151922Z",
     "iopub.status.busy": "2022-01-05T18:46:10.151507Z",
     "iopub.status.idle": "2022-01-05T18:46:10.162187Z",
     "shell.execute_reply": "2022-01-05T18:46:10.161397Z",
     "shell.execute_reply.started": "2022-01-05T18:46:10.151881Z"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM_imp(aspectClassificationBase):#model class for BI-LSTM\n",
    "    def __init__(self,emb_mat,num_classes,embedding_dim,vocab_size_t,vocab_size_a):\n",
    "        super(LSTM_imp,self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size_t = vocab_size_t\n",
    "        self.vocab_size_a=vocab_size_a\n",
    "        self.polarities=num_classes\n",
    "        self.embed = nn.Embedding.from_pretrained(emb_mat,freeze=True)\n",
    "        self.lstm =nn.LSTM(self.embedding_dim,128,batch_first=True,num_layers=2,dropout=0.2,bidirectional=True)\n",
    "        #self.lstm_a  =nn.LSTM(self.embedding_dim,128,batch_first=True,bidirectional=True)\n",
    "        self.dense = nn.Linear(self.vocab_size_t*self.vocab_size_a,self.polarities)\n",
    "        #self.softmax=nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, text,aspect):\n",
    "        #asp_raw_indices=inputs[:,278:,:]\n",
    "       # y1=inputs[0][278:][:]\n",
    "        #y2=inputs[1][278:][:]\n",
    "        \n",
    "        #text_raw_indices = torch.stack([x1,x2])\n",
    "        #asp_raw_indices=torch.stack([y1,y2])\n",
    "        #print(asp_raw_indices.shape)\n",
    "        #print(text_raw_indices.shape)\n",
    "        x= self.embed(text)\n",
    "        x,_=self.lstm(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x2=self.embed(aspect)\n",
    "        x2,_=self.lstm(x2)\n",
    "        #print(x2.shape)\n",
    "        x2=x2.permute(0,2,1)\n",
    "        \n",
    "        x3=torch.matmul(x,x2)\n",
    "        #print(x3.shape)\n",
    "        x3=x3.reshape(batch_size,-1)\n",
    "        x3=self.dense(x3)\n",
    "        #print(x3.shape)\n",
    "       \n",
    "        #x_len = torch.sum(text_raw_indices != 0, dim=-1)\n",
    "        #_, (h_n, _) = self.lstm(x, x_len)\n",
    "        #out = self.dense(h_n[0])\n",
    "        return x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T18:46:10.164184Z",
     "iopub.status.busy": "2022-01-05T18:46:10.163848Z",
     "iopub.status.idle": "2022-01-05T18:46:10.400001Z",
     "shell.execute_reply": "2022-01-05T18:46:10.399118Z",
     "shell.execute_reply.started": "2022-01-05T18:46:10.164147Z"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM_AOA(aspectClassificationBase):#model class for Attention over attention\n",
    "    def __init__(self,emb_mat,num_classes,embedding_dim,vocab_size_t,vocab_size_a):#emb_mat stores glove embeddings of all \n",
    "                                                                                   #words in our dictionary\n",
    "        super(LSTM_AOA,self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size_t = vocab_size_t\n",
    "        self.vocab_size_a=vocab_size_a\n",
    "        self.polarities=num_classes\n",
    "        self.embed = nn.Embedding.from_pretrained(emb_mat,freeze=True)#use glove word embedding matrix \n",
    "        self.lstm =nn.LSTM(self.embedding_dim,128,batch_first=True,num_layers=1,bidirectional=True)\n",
    "        #self.lstm_a  =nn.LSTM(self.embedding_dim,128,batch_first=True,bidirectional=True)\n",
    "        self.dense = nn.Linear(256,self.polarities)\n",
    "        self.softmax_c=nn.Softmax(dim=2)\n",
    "        self.softmax_r=nn.Softmax(dim=1)\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, text,aspect,batch_size):\n",
    "        #asp_raw_indices=inputs[:,278:,:]\n",
    "       # y1=inputs[0][278:][:]\n",
    "        #y2=inputs[1][278:][:]\n",
    "        \n",
    "        #text_raw_indices = torch.stack([x1,x2])\n",
    "        #asp_raw_indices=torch.stack([y1,y2])\n",
    "        #print(asp_raw_indices.shape)\n",
    "        #print(text_raw_indices.shape)\n",
    "        x= self.embed(text)\n",
    "        x,_=self.lstm(x)\n",
    "       # print(x.shape)\n",
    "        \n",
    "        x2=self.embed(aspect)\n",
    "        x2,_=self.lstm(x2)\n",
    "        #print(x2.shape)\n",
    "        x2=x2.permute(0,2,1)\n",
    "        \n",
    "        x3=torch.matmul(x,x2)\n",
    "        #print(x3.shape)\n",
    "        x4=x3.detach().clone()\n",
    "        x5=x3.detach().clone()\n",
    "        x5=self.softmax_c(x5)\n",
    "        #print(x5.shape)\n",
    "        x4=self.softmax_r(x4)\n",
    "        #x4=x4.permute(0,2,1)\n",
    "        x5=x5.sum(dim=1)/self.vocab_size_t\n",
    "        #print(x5.shape)\n",
    "        #print(x5)\n",
    "        x5=x5.reshape(batch_size,self.vocab_size_a,1)\n",
    "        x6=torch.matmul(x4,x5)\n",
    "     \n",
    "       # print(x6.shape)\n",
    "       # print(x6.shape)\n",
    "        x=x.permute(0,2,1)\n",
    "       # print(x.shape)\n",
    "        x7=torch.matmul(x,x6)\n",
    "        #print(x7.shape)\n",
    "        \n",
    "        \n",
    "        x7=x7.reshape(batch_size,-1)\n",
    "        #print(x7.shape)\n",
    "        x7=self.dense(x7)#no need for softmax if cross entropy loss is being used as a loss function.\n",
    "        #print(x7.shape)\n",
    "        #out=self.softmax(x7)\n",
    "        #x_len = torch.sum(text_raw_indices != 0, dim=-1)\n",
    "        #_, (h_n, _) = self.lstm(x, x_len)\n",
    "        #out = self.dense(h_n[0])\n",
    "        return x7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=get_default_device()#get available device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T19:20:05.992859Z",
     "iopub.status.busy": "2022-01-05T19:20:05.992237Z",
     "iopub.status.idle": "2022-01-05T19:20:20.351283Z",
     "shell.execute_reply": "2022-01-05T19:20:20.350570Z",
     "shell.execute_reply.started": "2022-01-05T19:20:05.992818Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T19:20:24.864651Z",
     "iopub.status.busy": "2022-01-05T19:20:24.864378Z",
     "iopub.status.idle": "2022-01-05T19:20:28.605456Z",
     "shell.execute_reply": "2022-01-05T19:20:28.604110Z",
     "shell.execute_reply.started": "2022-01-05T19:20:24.864621Z"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LSTM_AOA is a bit more accurate than LSTM_imp**\n",
    "AOA-66.8% \n",
    "LSTM_imp-62%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T19:15:56.168437Z",
     "iopub.status.busy": "2022-01-05T19:15:56.168183Z",
     "iopub.status.idle": "2022-01-05T19:16:05.451768Z",
     "shell.execute_reply": "2022-01-05T19:16:05.451081Z",
     "shell.execute_reply.started": "2022-01-05T19:15:56.168409Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name= input(\"Enter model\")\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T19:16:08.697925Z",
     "iopub.status.busy": "2022-01-05T19:16:08.697675Z",
     "iopub.status.idle": "2022-01-05T19:16:08.701692Z",
     "shell.execute_reply": "2022-01-05T19:16:08.700991Z",
     "shell.execute_reply.started": "2022-01-05T19:16:08.697897Z"
    }
   },
   "outputs": [],
   "source": [
    "if model_name=='LSTM_AOA':\n",
    "    b=20\n",
    "elif model_name=='LSTM_imp':\n",
    "    b=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T19:16:14.615579Z",
     "iopub.status.busy": "2022-01-05T19:16:14.614760Z",
     "iopub.status.idle": "2022-01-05T19:16:14.641269Z",
     "shell.execute_reply": "2022-01-05T19:16:14.640579Z",
     "shell.execute_reply.started": "2022-01-05T19:16:14.615531Z"
    }
   },
   "outputs": [],
   "source": [
    "if b==20:\n",
    "    model = torch.load('../input/d/enkrish259/aoa-colwise-softmax/aoa_b20_e30_entire_model.pth')#load AOA_Lstm  model\n",
    "elif b==2:\n",
    "    model=torch.load('../input/lstm-vocab/lstm_bi_e30.pth')#load lstm_imp model\n",
    "to_device(model,device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T17:41:39.649199Z",
     "iopub.status.busy": "2022-01-05T17:41:39.648652Z",
     "iopub.status.idle": "2022-01-05T17:41:39.653688Z",
     "shell.execute_reply": "2022-01-05T17:41:39.652426Z",
     "shell.execute_reply.started": "2022-01-05T17:41:39.649165Z"
    }
   },
   "outputs": [],
   "source": [
    "#vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T17:39:35.392659Z",
     "iopub.status.busy": "2022-01-05T17:39:35.391860Z",
     "iopub.status.idle": "2022-01-05T17:39:35.396576Z",
     "shell.execute_reply": "2022-01-05T17:39:35.395732Z",
     "shell.execute_reply.started": "2022-01-05T17:39:35.392617Z"
    }
   },
   "outputs": [],
   "source": [
    "#vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T17:38:09.408104Z",
     "iopub.status.busy": "2022-01-05T17:38:09.407773Z",
     "iopub.status.idle": "2022-01-05T17:38:09.411425Z",
     "shell.execute_reply": "2022-01-05T17:38:09.410811Z",
     "shell.execute_reply.started": "2022-01-05T17:38:09.408074Z"
    }
   },
   "outputs": [],
   "source": [
    "#idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T19:16:19.293215Z",
     "iopub.status.busy": "2022-01-05T19:16:19.292878Z",
     "iopub.status.idle": "2022-01-05T19:16:19.301604Z",
     "shell.execute_reply": "2022-01-05T19:16:19.300769Z",
     "shell.execute_reply.started": "2022-01-05T19:16:19.293180Z"
    }
   },
   "outputs": [],
   "source": [
    "#text encoding/vocab building\n",
    "def inf_prep(data):\n",
    "    vocab_file= open(\"../input/lstm-vocab/full_vocabulary.json\")\n",
    "    vocab_f=vocab_file.read()\n",
    "    vocab=json.loads(vocab_f)#load json file which stores words to index dictionary for our data\n",
    "    if type(data)==float:\n",
    "        data=str(data)\n",
    "    text2=data.lower()\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    tokens = tokenizer.tokenize(text2)\n",
    "    words = [word for word in tokens if word.isalnum()]\n",
    "    w_ind={v: int(k) for k, v in vocab.items()}\n",
    "   # print(w_ind)\n",
    "    encoded_sentence = np.array([w_ind[word] for word in words])#word to index embedded sentence\n",
    "    return encoded_sentence\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T19:16:21.668484Z",
     "iopub.status.busy": "2022-01-05T19:16:21.667749Z",
     "iopub.status.idle": "2022-01-05T19:16:21.672896Z",
     "shell.execute_reply": "2022-01-05T19:16:21.672117Z",
     "shell.execute_reply.started": "2022-01-05T19:16:21.668449Z"
    }
   },
   "outputs": [],
   "source": [
    "#encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T19:30:12.666890Z",
     "iopub.status.busy": "2022-01-05T19:30:12.666632Z",
     "iopub.status.idle": "2022-01-05T19:30:12.672653Z",
     "shell.execute_reply": "2022-01-05T19:30:12.671884Z",
     "shell.execute_reply.started": "2022-01-05T19:30:12.666862Z"
    }
   },
   "outputs": [],
   "source": [
    "#function in which model inference occurs\n",
    "def infer(test_dl,model,b):\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dl:\n",
    "            text,aspect=batch\n",
    "           # print(text)\n",
    "            #print(aspect)\n",
    "            out=model(text,aspect,b)#use this for AOA\n",
    "            #out=model(text,aspect) -use this one for LSTM_IMP\n",
    "            \n",
    "            _, preds = torch.max(out, dim=1)\n",
    "            #print(preds)\n",
    "            preds=preds.cpu().detach().numpy()\n",
    "            preds=preds.tolist()\n",
    "            return {'label':preds[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T19:31:16.776773Z",
     "iopub.status.busy": "2022-01-05T19:31:16.776073Z",
     "iopub.status.idle": "2022-01-05T19:31:16.785261Z",
     "shell.execute_reply": "2022-01-05T19:31:16.784379Z",
     "shell.execute_reply.started": "2022-01-05T19:31:16.776733Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_ans(text,aspect,b,device,model):\n",
    "    tex=np.empty([b,278])\n",
    "    aspec=np.empty([b,8])\n",
    "    encoded_text=inf_prep(text)\n",
    "    encoded_text=np.append(encoded_text,np.zeros(278-len(encoded_text)))\n",
    "    encoded_aspect=inf_prep(aspect)\n",
    "    encoded_aspect=np.append(encoded_aspect,np.zeros(8-len(encoded_aspect)))\n",
    "\n",
    "    for i in range(b):\n",
    "        tex[i]=encoded_text\n",
    "        aspec[i]=encoded_aspect\n",
    "    tex=torch.LongTensor(tex)\n",
    "    aspec=torch.LongTensor(aspec)\n",
    "    test_ds=TensorDataset(tex,aspec)\n",
    "    #input batch size=20 for LSTM_AOA and 2 for LSTM_imp\n",
    "    test_dl = DataLoader(test_ds, b, shuffle=False)\n",
    "    test_dl=DeviceDataLoader(test_dl,device)\n",
    "    ans=infer(test_dl,model,b)\n",
    "    if ans['label']==0:\n",
    "        print('Negative sentiment')\n",
    "    elif ans['label']==1:\n",
    "        print('Neutral sentiment')\n",
    "    elif ans['label']==2:\n",
    "        print('Positive sentiment')\n",
    "    return ans\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = input(\"Enter your text/context: \")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect= input(\"Enter aspect \")\n",
    "print(aspect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T19:31:19.318308Z",
     "iopub.status.busy": "2022-01-05T19:31:19.317782Z",
     "iopub.status.idle": "2022-01-05T19:31:19.357655Z",
     "shell.execute_reply": "2022-01-05T19:31:19.356838Z",
     "shell.execute_reply.started": "2022-01-05T19:31:19.318269Z"
    }
   },
   "outputs": [],
   "source": [
    "predict_ans(text,aspect,b,device,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
